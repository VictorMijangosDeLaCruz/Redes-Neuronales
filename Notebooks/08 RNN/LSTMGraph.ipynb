{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a6b564",
   "metadata": {},
   "source": [
    "# Long-Short Term Memories (LSTMs)\n",
    "\n",
    "Las Long-Short Term Memories o LSTMs incorporan celdas de \"memoria\" en las redes recurrentes. La idea esencial de las LSTMs es preservar la información relevante de los estados previos, mientras que son capaces de olvidar información innecesarias también de las capas preivas.\n",
    "\n",
    "Para implementar las LTMs haremos uso de las nociones de gráficas computacionales, definiendo un nodo LSTM. Dentro de este nodo se incorporarán el paso forward (el cálculo de los valores de salida) y el backward (la derivación y actualización de los pesos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ea6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d6c56",
   "metadata": {},
   "source": [
    "Ya que estaremos usando constantemente la función sigmoide dentro de las LSTMs definimos, en primer luagar, esta función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172f86f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    \"\"\"Función sigmoide\"\"\"\n",
    "    return 1./(1.+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753baf7b",
   "metadata": {},
   "source": [
    "## Capa de LSTM\n",
    "\n",
    "Para generar la capa de LSTM como un nodo en nuestra gráfica computacional, definiremos dos pasos: el forward y el backward. Asimismo, definiremos todos los pesos de las matrices de la LSTM. Recordemos que la LSTM cuenta con tres puertas $i, o$ y $f$, además genera un candidato $\\hat{c}$, donde cada una de estas puertas requiere de una matriz de pesos para los estados anteriores (recurrencias), una matriz de pesos para la entradas, y un bias. Es decir, requerimos 8 matrices de pesos y 4 bias. Estos se inicializan aleatoriamente.\n",
    "\n",
    "### Forward\n",
    "\n",
    "En el paso forward, se calculan los valores de salida de la red, para esto, se estiman primero las tres compuertas principales del LSTM:\n",
    "\n",
    "$$i_t = \\sigma(U^ix^{(t)} + V^ih^{(t-1)} + b^i)$$\n",
    "\n",
    "$$o_t = \\sigma(U^ox^{(t)} + V^oh^{(t-1)} + b^o)$$\n",
    "\n",
    "$$f_t = \\sigma(U^fx^{(t)} + V^fh^{(t-1)} + b^f)$$\n",
    "\n",
    "Asimismo, se define el candidato a sombra de la forma:\n",
    "\n",
    "$$\\hat{c}_t = \\tanh(U^cx^{(t)} + V^ch^{(t-1)} + b^c)$$\n",
    "\n",
    "La sombra (la información que se mandará al siguiente estado) se define a partir de estas compuertas de la siguiente forma:\n",
    "\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\hat{c}_t$$\n",
    "\n",
    "Es justamente aquí donde radica el paso central de las LSTM, pues se procesan los estados anteriores multiplicando por la compuerta $f_t$ que se encargará de filtrar aquellas cosas que sean relevantes para el estado actual. Asimismo, la información actual, almacenada en $\\hat{c}_t$ se multiplicará con la compuerta $i_t$ que se enfocará en \"escribir\" la información relevante para el estado.\n",
    "\n",
    "Finalmente, en la celda de salida para este estado se aplicará la función de tangente hiperbólica para escalar los valores en $c_t$ y se \"leerá\" por medio de la compuerta $o_t$. Tenemos entonces:\n",
    "\n",
    "$$h_t = o_t \\odot \\tanh(c_t)$$\n",
    "\n",
    "<img src=\"imagenes/LSTM.png\" alt=\"Algoritmo de aprendizaje\" width=\"30%\">\n",
    "\n",
    "### Backward\n",
    "\n",
    "El backward es quizá el paso más pesado en una LSTM. En este paso, se deben estimar las derivadas de la capa para actualizar los pesos por medio de un optimizador basado en descenso por gradiente. En las redes recurrentes, recordemos que las función de riesgo (u objetivo) considera la suma sobre los diferentes tiempos:\n",
    "\n",
    "$$R(\\theta) = \\sum_t \\sum_y L_t(f_y(x)^{(t)}, y^{(t)})$$\n",
    "\n",
    "Derivar sobre los pesos de la capa LSTM implicará, primero, entrar por medio de la retro-propagación, a esta capa. Para derivar sobre cada uno de los pesos de LSTM, tenemos que considerar el orden en que se ejecutan; es decir, debemos fijarnos en cómo se ejecuta la gráfica computacional. \n",
    "\n",
    "En primer lugar, estaremos almacenando una variable que acumule las derivadas en los estados superiores de las recurrencias. Esta variable será de la forma:\n",
    "\n",
    "$$d_{st} = \\sum_t \\sum_y \\frac{\\partial L_t}{\\partial h_t} o_t(1-tanh(c_t)^2)$$\n",
    "\n",
    "Donde el término $\\sum_y \\frac{\\partial L_t}{\\partial h_t}$ son las derivadas en las capas superiores en el tiempo $t$.\n",
    "\n",
    "Podemos ver que los pesos que están más arriba en esta gráfica son los que corresponden a la compuerta $o_t$, pues ésta es la que se encuentra en la salida de la capa; entonces, podemos empezar derivando sobre los pesos en esta capa:\n",
    "\n",
    "$$\\frac{\\partial R(\\theta)}{\\partial V_i^o} = \\sum_y \\frac{\\partial L_t}{\\partial h_t}  o_t (1-o_t) \\odot \\tanh(c_t) h_i^{t-1}$$\n",
    "\n",
    "El término $\\frac{\\partial L_t}{\\partial h_t}$ es justamente la derivada en la capa superior. Por lo que sólo tenemos que fijarnos en la última parte. Como $o_t$ es la función sigmoide, su derivada es $o_t(1-o_t)$ el otro término se toma como constante. En el caso de derivar sobre $U^{(o)}$, sólo cambia el último término por $x^{(t)}$.\n",
    "\n",
    "La siguiente operación que ejecuta la capa LSTM es el cálculo de $c_t$ en donde entran en juego las compuertas restantes. Aquí podemos derivar las compuertas en cualquier orden, y cada una de ellas tendrá una apariencia similar. Tomemos los siguientes casos:\n",
    "\n",
    "* Candidato $\\hat{c}_t$: $$\\frac{\\partial R(\\theta}{\\partial V^c_i} = d_{st}\\odot i_t \\odot(1-\\hat{c}_t^2) h_i^{(t-1} $$\n",
    "* Compuerta escritura $i_t$: $$\\frac{\\partial R(\\theta}{\\partial V^i_i} = d_{st}\\odot i_t(1-i_t) \\hat{c} h_i^{(t-1} $$\n",
    "* Compuerta olvido $f_t$: $$\\frac{\\partial R(\\theta}{\\partial V^f_i} = d_{st}\\odot f_t(1-f_t) c_{t-1} h_i^{(t-1} $$\n",
    "\n",
    "Para los casos de las matrices $U^f, U^i, U^c$ las derivadas sólo cmabian en el último término por el vector de entrada $x^{(t)}$. En los bias, este mismo término cambia por un valor 1. Por tanto, estos términos se guardarán como variables para usarse en la derivación de los términos.\n",
    "\n",
    "En nuestra implementación, en lugar de acumular la derivada a través de los tiempos, actualizamos los pesos en cada tiempo $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa429e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM():\n",
    "    \"\"\"Clase para crear un nodo LSTM\"\"\"\n",
    "    def __init__(self, input_size, output_size, h0=None, c0=None):\n",
    "        super(LSTM,self).__init__\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        #Celda de escritura\n",
    "        self.Vi = np.random.randn(output_size,output_size) / np.sqrt(output_size)\n",
    "        self.Ui = np.random.randn(output_size, input_size) / np.sqrt(input_size)\n",
    "        self.bi = np.zeros(output_size)\n",
    "        #Celda de olvido\n",
    "        self.Vf = np.random.randn(output_size,output_size) / np.sqrt(output_size)\n",
    "        self.Uf = np.random.randn(output_size, input_size) / np.sqrt(input_size)\n",
    "        self.bf = np.zeros(output_size)\n",
    "        #Celda de lectura\n",
    "        self.Vo = np.random.randn(output_size,output_size) / np.sqrt(output_size)\n",
    "        self.Uo = np.random.randn(output_size, input_size) / np.sqrt(input_size)\n",
    "        self.bo = np.zeros(output_size)\n",
    "        #Celda de candidato\n",
    "        self.Vc = np.random.randn(output_size,output_size) / np.sqrt(output_size)\n",
    "        self.Uc = np.random.randn(output_size, input_size) / np.sqrt(input_size)\n",
    "        self.bc = np.zeros(output_size)\n",
    "        #Entrada\n",
    "        self.x = None\n",
    "        #Vector de inicialización\n",
    "        if h0 == None and c0 == None:\n",
    "            self.h0,self.c0 = np.zeros(output_size),np.zeros(output_size)\n",
    "        else:\n",
    "            self.h0,self.c0 = h0,c0\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        l = x.shape[0]\n",
    "        #Inicializacion de celdas\n",
    "        self.h = np.zeros((l+1,self.output_size))\n",
    "        self.h[0] = self.h0\n",
    "        #Inicializacion de sombras\n",
    "        self.c = np.zeros((l+1,self.output_size))\n",
    "        self.c[0] = self.c0\n",
    "        #Inicialización puertas\n",
    "        self.i = np.zeros((l,self.output_size))\n",
    "        self.f = np.zeros((l,self.output_size))\n",
    "        self.o = np.zeros((l,self.output_size))\n",
    "        self.c_hat = np.zeros((l,self.output_size))\n",
    "        \n",
    "        for t, x_t in enumerate(x):\n",
    "            #Aplicación  del forward\n",
    "            self.i[t] = sig(self.Vi@self.h[t] + self.Ui@x_t + self.bi)\n",
    "            self.f[t] = sig(self.Vf@self.h[t] + self.Uf@x_t + self.bf)\n",
    "            self.o[t] = sig(self.Vo@self.h[t] + self.Uo@x_t + self.bo)\n",
    "            self.c_hat[t] = np.tanh(self.Vc@self.h[t] + self.Uc@x_t + self.bc)\n",
    "    \n",
    "            self.c[t+1] = self.f[t]*self.c[t] + self.i[t]*self.c_hat[t]\n",
    "            self.h[t+1] = self.o[t]*np.tanh(self.c[t+1])\n",
    "            \n",
    "        return self.h[1:], self.c[1:]\n",
    "    \n",
    "    def backward(self,layer,lr=0.1):\n",
    "        #Variable de estado\n",
    "        d_t = np.zeros(self.output_size)\n",
    "        self.w = []\n",
    "        self.d = []\n",
    "        for t in range(self.x.shape[0])[::-1]:\n",
    "            prev_d = layer.d[t]\n",
    "            #Variable de lectura\n",
    "            d_o = prev_d*np.tanh(self.c[t+1])*self.o[t]*(1-self.o[t])\n",
    "            dVo = np.outer(d_o,self.h[t])\n",
    "            dUo = np.outer(d_o,self.x[t])\n",
    "            self.Vo -= lr*dVo\n",
    "            self.bo -= lr*d_o\n",
    "            self.Uo -= lr*dUo\n",
    "            #Varible de estado (se usa en escritura, olvido y sombra)\n",
    "            d_st = prev_d*self.o[t]*(1-np.tanh(self.c[t+1])**2) + d_t\n",
    "            #Variable de sombra\n",
    "            d_c = d_st*self.i[t]*(1-self.c_hat[t]**2)\n",
    "            dVc = np.outer(d_c,self.h[t])\n",
    "            dUc = np.outer(d_c,self.x[t])\n",
    "            self.Vc -= lr*dVc\n",
    "            self.bc -= lr*d_c\n",
    "            self.Uc -= lr*dUc\n",
    "            #Variable de escritura\n",
    "            d_i = d_st*self.c_hat[t]*self.i[t]*(1-self.i[t])\n",
    "            dVi = np.outer(d_i,self.h[t])\n",
    "            dUi = np.outer(d_i,self.x[t])\n",
    "            self.Vi -= lr*dVi\n",
    "            self.bi -= lr*d_i\n",
    "            self.Ui -= lr*dUi\n",
    "            #Variable de olvido\n",
    "            d_f = d_st*self.c[t]*self.f[t]*(1-self.f[t])\n",
    "            dVf = np.outer(d_f,self.h[t])\n",
    "            dUf = np.outer(d_f,self.x[t])\n",
    "            self.Vf -= lr*dVf\n",
    "            self.bf -= lr*d_f\n",
    "            self.Uf -= lr*dUf\n",
    "\n",
    "            self.d.append( dUo.T@d_o + dUi.T@d_i + dUc.T@d_c + dUf.T@d_f )\n",
    "            \n",
    "            #Nueva variable de cambio\n",
    "            d_t = self.f[t]*d_st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e99040",
   "metadata": {},
   "source": [
    "## Aplicación de la LSTM\n",
    "\n",
    "Para mostrar el funcionamiento de nuestra LSTM definiremos un problema simple de lenguaje natural: este problema es conocido como el etiquetado de partes de la oración (POS por sus siglas en inglés). En este se busca identificar la correspondiente categoría gramátical (o tipo de palabra) de una palabra en una oración. Por ejemplo \"el\", \"la\", \"los\" son artículos (DA), mientras que \"comer\", \"salto\" son verbos (V). \n",
    "\n",
    "Utilizamos un toy dataset para ejemplificar este problema. El problema requiere de un preprocesamiento en que las palabras y las etiquetas se indexan. De hecho, como en muchos modelos orientados al lenguaje natural requerimos usar una capa de Embedding, que ya hemos definido previamente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3da597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toy dataset\n",
    "inputs = ['el perro come un hueso', 'un muchacho jugaba', 'el muchacho saltaba la cuerda', 'el perro come mucho',\n",
    "          'un perro come croquetas', 'el perro come', 'el gato come croquetas', 'un gato come', 'yo juego mucho', \n",
    "          'el juego', 'un juego', 'yo juego un juego', 'el gato come mucho']\n",
    "outputs = ['DA NC V DD NC', 'DD NC V', 'DA NC V DA NC', 'DD NC V NC', 'DA NC V Adv', 'DA NC V', 'DA NC V NC', \n",
    "           'DD NC V', 'DP V Adv', 'DA NC', 'DD NC', 'DP V DD NC', 'DA NC V Adv']\n",
    "\n",
    "#Indexación de los datos\n",
    "in_voc = vocab()\n",
    "x = list(text2numba(inputs,in_voc))\n",
    "out_voc = vocab()\n",
    "y = list(text2numba(outputs,out_voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb73a55",
   "metadata": {},
   "source": [
    "Ahora generamos nuestra red a partir de las siguientes capas:\n",
    "\n",
    "1. Capa de Embedding: generará los vectores de palabras a partir d elos índices de éstas.\n",
    "2. Capa LSTM para procesar secuencialmente la entrada.\n",
    "3. Capa de salida, con activación Softmax para elegir la etiqueta con mayor probabilidad.\n",
    "\n",
    "Finalmente, nuestra función de riesgo será la entropía cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "020a3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capa de Embedding\n",
    "emb = Embedding(len(in_voc),100)\n",
    "#Capa LSTM\n",
    "lstm = LSTM(100,200)\n",
    "#Capa Lineal\n",
    "lin = Linear(200,len(out_voc))\n",
    "#Activación Softmax\n",
    "soft = Softmax(normalize=True)\n",
    "#Función de riesgo\n",
    "risk = CrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b465f",
   "metadata": {},
   "source": [
    "Finalmente, procedemos a entrenar nustra red con base en el descenso por gradiente. Elegimos un número de épocas adecuado y una tasa de aprendizaje adecuada. Las actualizaciones se hacen dentro del mismo método <tt>backward</tt> en cada capa de la red-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dffb21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 300/300 [00:26<00:00, 11.52it/s]\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-2\n",
    "epochs = 300\n",
    "for t in tqdm(range(epochs)):\n",
    "    for x_i, y_i in zip(x,y):\n",
    "        #Forward\n",
    "        e = emb(x_i)\n",
    "        h1,c = lstm(e)\n",
    "        a = lin(h1)\n",
    "        f = soft(a)\n",
    "        loss = risk(y_i,f)\n",
    "\n",
    "        #Backward\n",
    "        risk.backward()\n",
    "        soft.backward(risk)\n",
    "        lin.backward(soft, lr=lr)\n",
    "        lstm.backward(lin, lr=lr)\n",
    "        emb.backward(lstm, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc9c0e",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "\n",
    "Para probar los resultados de nuestra red, construimos una funciónn <tt>tagger</tt> que nos permitirá etiquetar una oración por medio de nuestra red neuronal LSTM. El proceso es simple, pues toma la cadena, separa las palabras, las indexa y ésta es la entrada de nuestra red. En la salida sólo se recuperan las etiquetas correspondientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e66047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {i:tag for tag,i in out_voc.items()}\n",
    "def tagger(sent):\n",
    "    \"\"\"Función de etiquetado con LSTM\"\"\"\n",
    "    x = [in_voc[w] for w in sent.split()]\n",
    "    p = soft(lin(lstm(emb(x))[0]))\n",
    "    y_pred = p.argmax(1)\n",
    "    \n",
    "    return ' '.join([tags[j] for j in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa538a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el muchacho come un perro\n",
      "DA NC V DD NC\n"
     ]
    }
   ],
   "source": [
    "sent = 'el muchacho come un perro'\n",
    "result = tagger(sent)\n",
    "print(sent)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeecaf4c",
   "metadata": {},
   "source": [
    "En este caso, DA corresponde a artículo, NC a nombre común, V a verbo, DD a demostrativo (\"un\", \"una\") y Adv a adverbio. Se observar que el etiquetado, en este ejemplo de juguete es satisfactorio. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
