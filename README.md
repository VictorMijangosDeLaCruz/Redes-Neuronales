# Redes Neuronales

Este repositorio contiene material del curso de Redes neuronales, en la Facultad de Ciencias, UNAM. Y ha sido desarrollado por varios contribuidores bajo el proyecto PAPIME PE102723.
Las redes neuronales artificiales son modelo de aprendizaje estadístico que buscan emular la forma en que una neurona procesa información. Tradicionalmente se enfocan a tareas de clasificación, pero también pueden resolver otras tareas como regresión o, incluso, tareas no supervisadas o generativas.

## Objetivo del curso: 
El curso comprenderá el modelo matemático de la red neuronal que da pie a las redes neuronales artificiales y su generalización. Se revisarán los temas más prominentes sobre redes neuronales: su modelación, el concepto de arquitectura y el proceso de aprendizaje que conllevan.
Asimismo, se profundizará en algoritmos específicos de la familia de redes neuronales, como es el Perceptrón, las redes FeedForward, las redes recurrentes, entre otras. Dentro de esto, se buscará comprender los diferentes problemas que corresponden a cada uno de estos modelos (problemas secuenciales, de clasificación, de regresión, de estimación probabilística).
También se profundizará en la implementación de las redes neuronales, tanto la programación de los módulos básicos, como el uso de paquetería especializada en estos (tensorflow, pytorch).
Además de los algoritmos típicos dentro de la familia de redes neuronales, se impulsará la creación de nuevas arquitecturas que respondan a problemas específicos. Se abordará también la teoría de aprendizaje geométrico profundo que plantea un marco matemático para el planteamiento de diferentes arquitecturas a problemas estructurados.

## Temas

1. Introducción
- &nbsp; 1.1. [Teoría de aprendizaje estadístico](https://victormijangosdelacruz.github.io/Redes-Neuronales/html/Introduccion/00AprendizajeMaquina.html)
- &nbsp; 1.2. [Representación de datos](https://victormijangosdelacruz.github.io/Redes-Neuronales/html/Introduccion/01RepresentacionDatos.html)
- &nbsp; 1.3. [Modelo biológico de la neurona](https://github.com/VictorMijangosDeLaCruz/Redes_Neuronales/blob/main/Notebooks/00%20Modelo%20de%20Hudgkin-Huxley)
3. Perceptrón
- &nbsp; 3.1. Definición del perceptrón
- &nbsp; 3.2. Regla de aprendizaje en el perceptrón
- &nbsp; 3.3. Teorema de convergencia
- &nbsp; 3.4. Funciones lógicas y límites del perceptrón
4. Redes FeedForward
- &nbsp; 4.1. Capas ocultas en las redes neuronales
- &nbsp; 4.2. Funciones de activación
- &nbsp; 4.3. Teoremas del aproximador universal
- &nbsp; 4.4. Complejidad de redes FeedForward
- &nbsp; 4.5. Redes profundas
5. Aprendizaje en redes multicapa
- &nbsp; 5.1. Problema de aprendizaje en redes multi-capa
- &nbsp; 5.2. Gradiente descendiente
- &nbsp; 5.3. Otros optimizadores (Adagrad, Adam, RMSProp,...)
- &nbsp; 5.4. Backpropagation
- &nbsp; 5.5. Aceleración del entrenamiento mediante GPU
- &nbsp; 5.6. Métodos de regularización (Tychonoff, Dropout)
6. Redes neuronales recurrentes
- &nbsp; 6.1. Capas recurrentes
- &nbsp; 6.2. Backpropagation Through Time
- &nbsp; 6.3. Tipos de redes recurrentes
- &nbsp; 6.4. Redes bidireccionales
- &nbsp; 6.5. Long-Short Term Memory (LSTM) y Gated Recurrent Unit (GRU)
7. Redes convolucionales
- &nbsp; 7.1. Convoluciones y correlación cruzada
- &nbsp; 7.2. Redes convolucionales
8. Redes atencionales
- &nbsp; 8.1. Atención
- &nbsp; 8.2. Transformers


## Colaboradores

[@veroarriola](https://github.com/veroarriola)

[@milmor](https://github.com/milmor)

[@VictorMijangosDeLaCruz](https://github.com/VictorMijangosDeLaCruz)


---------------------------------------------------------------------------------
<div style="text-align: right">Proyecto PAPIME PE102723</div>
